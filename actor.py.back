# Copyright 2019 DeepMind Technologies Limited
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Python implementation of R-NaD (https://arxiv.org/pdf/2206.15378.pdf)."""
import random
import sys
import enum
import functools
import time
from typing import Any, Callable, Sequence, Tuple, Union, Mapping, Optional
import chex
import haiku as hk
import jax
from jax import lax
from jax import numpy as jnp
from jax import tree_util as tree
import ray
import tensorflow as tf

import numpy as np
import optax

from open_spiel.python import policy as policy_lib
import pyspiel
from network.cnn_paper_32 import PyramidModule, ValueHeadModule, PolicyHeadModule
#from rnad.data_class import *
#from rnad.entropy_schedule import EntropySchedule
#from rnad.func import *
from rnad.rnad import _legal_policy, _player_others, _policy_ratio, _where, _has_played
from rnad.rnad import *

# Some handy aliases.
# Since most of these are just aliases for a "bag of tensors", the goal
# is to improve the documentation, and not to actually enforce correctness
# through pytype.
Params = chex.ArrayTree

@ray.remote(num_cpus=1,num_gpus=0)
class RNaDActor(RNaDSolver):
    """Implements a solver for the R-NaD Algorithm.

  See https://arxiv.org/abs/2206.15378.

  Define all networks. Derive losses & learning steps. Initialize the game
  state and algorithmic variables.
  """

    def init(self):
        """Initialize the network and losses."""
        # The random facilities for jax and numpy.
        self._rngkey = jax.random.PRNGKey(self.config.seed)
        self._np_rng = np.random.RandomState(self.config.seed)
        # TODO(author16): serialize both above to get the fully deterministic behaviour.

        # Create a game and an example of a state.
        self._game = pyspiel.load_game(self.config.game_name)
        self._ex_state = self._play_chance(self._game.new_initial_state())
        
        # The network.
        def network(env_step) -> Tuple[chex.Array, chex.Array, chex.Array, chex.Array]:
            conv_torso = PyramidModule(N=2, M=2)  # env_step.obs

            dm0 = self.config.batch_size if env_step.obs.shape != (1460,) else 1
            shape = (dm0, 10, 2, 73)
            reshaped_obs = jnp.reshape(env_step.obs, shape)

            torso = conv_torso(reshaped_obs)

            pm_value_head = ValueHeadModule()
            pm_policy_head = PolicyHeadModule()

            logit = pm_policy_head(torso)
            logit = jnp.reshape(logit, (dm0, 10*2))
            v = pm_value_head(torso)
            v = jnp.reshape(v, (dm0, 10*2))

            pi = _legal_policy(logit, env_step.legal)
            log_pi = legal_log_policy(logit, env_step.legal)
            return pi, v, log_pi, logit
        
        self.network = hk.without_apply_rng(hk.transform(network))

        # The machinery related to updating parameters/learner.
        self._entropy_schedule = EntropySchedule(
            sizes=self.config.entropy_schedule_size,
            repeats=self.config.entropy_schedule_repeats)
        self._loss_and_grad = jax.value_and_grad(self.loss, has_aux=False)

        # Create initial parameters.
        env_step = self._state_as_env_step(self._ex_state)
        key = self._next_rng_key()  # Make sure to use the same key for all.
        self.params = self.network.init(key, env_step)
        self.params_target = self.network.init(key, env_step)
        self.params_prev = self.network.init(key, env_step)
        self.params_prev_ = self.network.init(key, env_step)

        # Parameter optimizers.
        self.optimizer = optax_optimizer(
            self.params,
            optax.chain(
                optax.scale_by_adam(
                    eps_root=0.0,
                    **self.config.adam,
                ), optax.scale(-self.config.learning_rate),
                optax.clip(self.config.clip_gradient)))
        self.optimizer_target = optax_optimizer(
            self.params_target, optax.sgd(self.config.target_network_avg))

    def act(self):
        return self.collect_batch_trajectory()

    def _state_as_env_step(self, state: pyspiel.State) -> EnvStep:
        # A terminal state must be communicated to players, however since
        # it's a terminal state things like the state_representation or
        # the set of legal actions are meaningless and only needed
        # for the sake of creating well a defined trajectory tensor.
        # Therefore the code below:
        # - extracts the rewards
        # - if the state is terminal, uses a dummy other state for other fields.
        rewards = np.array(state.returns(), dtype=np.float64)

        valid = not state.is_terminal()
        if not valid:
            state = self._ex_state

        if self.config.state_representation == StateRepresentation.OBSERVATION:
            obs = state.observation_tensor()
        elif self.config.state_representation == StateRepresentation.INFO_SET:
            obs = state.information_state_tensor()
        else:
            raise ValueError(
                f"Invalid StateRepresentation: {self.config.state_representation}.")

        # TODO(author16): clarify the story around rewards and valid.
        return EnvStep(
            obs=np.array(obs, dtype=np.float64),
            legal=np.array(state.legal_actions_mask(), dtype=np.int8),
            player_id=np.array(state.current_player(), dtype=np.float64),
            valid=np.array(valid, dtype=np.float64),
            rewards=rewards)

    def actor_step(self, env_step: EnvStep):
        pi = self._network_jit_apply(self.params, env_step)
        pi = np.asarray(pi).astype("float64")
        # TODO(author18): is this policy normalization really needed?
        pi = pi / np.sum(pi, axis=-1, keepdims=True)

        action = np.apply_along_axis(
            lambda x: self._np_rng.choice(range(pi.shape[1]), p=x), axis=-1, arr=pi)
        # TODO(author16): reapply the legal actions mask to bullet-proof sampling.
        action_oh = np.zeros(pi.shape, dtype="float64")
        action_oh[range(pi.shape[0]), action] = 1.0

        actor_step_list = [pi, action_oh] # [B, A] # [[[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]],[[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]]]

        return action, actor_step_list

    def collect_batch_trajectory(self):
        states = [
            self._play_chance(self._game.new_initial_state())
            for _ in range(self.config.batch_size)
        ]
        timesteps = []

        env_step, env_step_list = self._batch_of_states_as_env_step(states)
        for _ in range(self.config.trajectory_max):
            prev_env_step_list = env_step_list
            a, actor_step_list = self.actor_step(env_step)

            states = self._batch_of_states_apply_action(states, a)
            env_step, env_step_list = self._batch_of_states_as_env_step(states)
            timesteps.append([prev_env_step_list, actor_step_list, env_step_list])
        # Concatenate all the timesteps together to form a single rollout [T, B, ..]
        # return jax.tree_util.tree_map(lambda *xs: np.stack(xs, axis=0), *timesteps)
        return timesteps

    def _batch_of_states_as_env_step(self,
                                     states: Sequence[pyspiel.State]) -> EnvStep:
        envs = [self._state_as_env_step(state) for state in states]
        return jax.tree_util.tree_map(lambda *e: np.stack(e, axis=0), *envs), envs

    def _batch_of_states_apply_action(
            self, states: Sequence[pyspiel.State],
            actions: chex.Array) -> Sequence[pyspiel.State]:
        """Apply a batch of `actions` to a parallel list of `states`."""
        for state, action in zip(states, list(actions)):
            if not state.is_terminal():
                self.actor_steps += 1
                state.apply_action(action)
                self._play_chance(state)
        return states

